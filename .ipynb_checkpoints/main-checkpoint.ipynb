{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import package\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from numpy import genfromtxt\n",
    "from math import sqrt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method joins/subtracts the input features\n",
    "def concatenate ( arg1, arg2,arg3):\n",
    "    sol = arg1 + arg2\n",
    "    sol.append(int(arg3))\n",
    "    return sol\n",
    "\n",
    "def subtraction ( arg1, arg2, arg3):\n",
    "    sol = [i - j for i, j in zip(arg1, arg2)]\n",
    "    sol.append(int(arg3))\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This method saves the shuffled and cleaned file in the target location\n",
    "def save_file(filename, content):\n",
    "    with open(filename, 'w', newline='') as fi:\n",
    "        writer = csv.writer(fi)\n",
    "        writer.writerows(content)\n",
    "    fid = open(filename, \"r\")\n",
    "    li = fid.readlines()\n",
    "    fid.close()\n",
    "    random.shuffle(li)\n",
    "    fid = open(filename, \"w\")\n",
    "    fid.writelines(li)\n",
    "    fid.close()\n",
    "    print(str(filename) + \" is created for \" + str(len(content)) + \" entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This method extracts the features from Human observed dataset\n",
    "def human_feature():\n",
    "    raw_data = []\n",
    "    featureid = []\n",
    "    featureset = []\n",
    "    with open(\"./Dataset/HumanObserved/HumanObserved-Features-Data.csv\", 'r') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader: \n",
    "            x = []\n",
    "            featureid.append(row[1])\n",
    "            raw_data.append(row)   \n",
    "            for i in range(9):\n",
    "                x.append(float(row[i+2]))\n",
    "            featureset.append(x)\n",
    "    return featureid,featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This method extracts the features from for same and different files and saves them in the specified path\n",
    "def human_concat_sub(featureid,featureset):\n",
    "    concatenation_raw_data = []\n",
    "    subtraction_raw_data = []\n",
    "    with open(\"Dataset/HumanObserved/same_pairs.csv\", 'r') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader:\n",
    "            sign1 = featureset[featureid.index(row[0])]\n",
    "            sign2 = featureset[featureid.index(row[1])]\n",
    "            sign3 = row[2]\n",
    "            concatenation_raw_data.append(concatenate(sign1,sign2,sign3))\n",
    "            subtraction_raw_data.append(subtraction(sign1,sign2,sign3))\n",
    "    with open(\"Dataset/HumanObserved/diffn_pairs.csv\", 'r') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader:\n",
    "            sign1 = featureset[featureid.index(row[0])]\n",
    "            sign2 = featureset[featureid.index(row[1])]\n",
    "            sign3 = row[2]     \n",
    "            concatenation_raw_data.append(concatenate(sign1,sign2,sign3))\n",
    "            subtraction_raw_data.append(subtraction(sign1,sign2,sign3))\n",
    "    save_file(\"Dataset-cleaned/human-feature-concatenation.csv\",concatenation_raw_data)\n",
    "    save_file(\"Dataset-cleaned/human-feature-subtraction.csv\",subtraction_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This method extracts the features from Gradient Stochastic concavity  dataset\n",
    "def gsc_feature():\n",
    "    raw_data = []\n",
    "    featureid = []\n",
    "    featureset = []\n",
    "    with open(\"./Dataset/GradientStructuralConcavity/GSC-Features.csv\", 'r') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader: \n",
    "            x = []\n",
    "            featureid.append(row[0])\n",
    "            raw_data.append(row)   \n",
    "            for i in range(512):\n",
    "                x.append(float(row[i+1]))\n",
    "            featureset.append(x)\n",
    "    return featureid,featureset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This method extracts the features from for same and different files and saves them in the specified path\n",
    "def gsc_concat_sub(featureid,featureset):\n",
    "    concatenation_raw_data = []\n",
    "    subtraction_raw_data = []\n",
    "    with open(\"Dataset/GradientStructuralConcavity/same_pairs.csv\", 'r') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader:\n",
    "            sign1 = featureset[featureid.index(row[0])]\n",
    "            sign2 = featureset[featureid.index(row[1])]\n",
    "            sign3 = row[2]\n",
    "            concatenation_raw_data.append(concatenate(sign1,sign2,sign3))\n",
    "            subtraction_raw_data.append(subtraction(sign1,sign2,sign3))\n",
    "    with open(\"Dataset/GradientStructuralConcavity/diffn_pairs.csv\", 'r') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader:\n",
    "            sign1 = featureset[featureid.index(row[0])]\n",
    "            sign2 = featureset[featureid.index(row[1])]\n",
    "            sign3 = row[2]     \n",
    "            concatenation_raw_data.append(concatenate(sign1,sign2,sign3))\n",
    "            subtraction_raw_data.append(subtraction(sign1,sign2,sign3))\n",
    "    save_file(\"Dataset-cleaned/gsc-feature-concatenation.csv\",concatenation_raw_data)\n",
    "    save_file(\"Dataset-cleaned/gsc-feature-subtraction.csv\",subtraction_raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This method extracts all features from Human observed and GSC dataset. It returns the feature and target set for selected datasets\n",
    "def data_extraction():\n",
    "    human_feature_id,human_feature_val  = human_feature()\n",
    "    human_concat_sub(human_feature_id,human_feature_val)\n",
    "\n",
    "    gsc_feature_id,gsc_feature_val  = gsc_feature()\n",
    "    gsc_concat_sub(gsc_feature_id,gsc_feature_val)\n",
    "\n",
    "    return human_feature_id,human_feature_val,gsc_feature_id ,gsc_feature_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-cleaned/human-feature-concatenation.csv is created for 1582 entries\n",
      "Dataset-cleaned/human-feature-subtraction.csv is created for 1582 entries\n",
      "Dataset-cleaned/gsc-feature-concatenation.csv is created for 10000 entries\n",
      "Dataset-cleaned/gsc-feature-subtraction.csv is created for 10000 entries\n"
     ]
    }
   ],
   "source": [
    "human_feature_id,human_feature_val,gsc_feature_id ,gsc_feature_val = data_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method reads the target file for all samples and appends the content to variable, and returns it as a list\n",
    "def GetTargetVector(filePath,position):\n",
    "    t = []\n",
    "    with open(filePath, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:  \n",
    "            t.append(int(row[int(position)]))\n",
    "    print(\" Target generated for  \" + str(filePath))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset is seperated into training, validation and test. \n",
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "# Regularization parameter to minimize error \n",
    "C_Lambda = 0.02\n",
    "#M is the number of basis function\n",
    "M = 4\n",
    "# φ is a vector of M basis functions\n",
    "PHI = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Target generated for  Dataset-cleaned/human-feature-concatenation.csv\n",
      " Target generated for  Dataset-cleaned/human-feature-subtraction.csv\n",
      " Target generated for  Dataset-cleaned/gsc-feature-concatenation.csv\n",
      " Target generated for  Dataset-cleaned/gsc-feature-subtraction.csv\n"
     ]
    }
   ],
   "source": [
    "RawTarget_human_concatenation = GetTargetVector('Dataset-cleaned/human-feature-concatenation.csv','18')\n",
    "RawTarget_human_subtraction = GetTargetVector('Dataset-cleaned/human-feature-subtraction.csv','9')\n",
    "RawTarget_gradient_concatenation = GetTargetVector('Dataset-cleaned/gsc-feature-concatenation.csv','1024')\n",
    "RawTarget_gradient_subtraction = GetTargetVector('Dataset-cleaned/gsc-feature-subtraction.csv','512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A matrix is built for the feature set. \n",
    "def GenerateRawData(filePath,position):  \n",
    "    dataMatrix = [] \n",
    "    with open(filePath, 'r') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader:\n",
    "            dataRow = []   \n",
    "            for i in range(int(position)):\n",
    "                dataRow.append(float(row[i]))\n",
    "            dataMatrix.append(dataRow)   \n",
    "    \n",
    "    dataMatrix = np.transpose(dataMatrix)        \n",
    "    print(\" Raw data generated for  \" + str(filePath))\n",
    "    return dataMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Raw data generated for  Dataset-cleaned/human-feature-concatenation.csv\n",
      " Raw data generated for  Dataset-cleaned/human-feature-subtraction.csv\n",
      " Raw data generated for  Dataset-cleaned/gsc-feature-concatenation.csv\n",
      " Raw data generated for  Dataset-cleaned/gsc-feature-subtraction.csv\n"
     ]
    }
   ],
   "source": [
    "RawData_human_concatenation = GenerateRawData('Dataset-cleaned/human-feature-concatenation.csv','18')\n",
    "RawData_human_subtraction = GenerateRawData('Dataset-cleaned/human-feature-subtraction.csv','9')\n",
    "RawData_gradient_concatenation = GenerateRawData('Dataset-cleaned/gsc-feature-concatenation.csv','1024')\n",
    "RawData_gradient_subtraction = GenerateRawData('Dataset-cleaned/gsc-feature-subtraction.csv','512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method gets the target value for training set appends the content to variable, and returns it as a list\n",
    "# 80% of the actual data (test data) is used here, to compose Traininglength\n",
    "def GenerateTrainingTarget(rawTraining):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    return t\n",
    "\n",
    "# This method constructs the design matrix for training data\n",
    "# 80% of the actual data (test data) is used construct the feature datamatrix\n",
    "def GenerateTrainingDataMatrix(rawData):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    t = rawData[:,0:T_len]\n",
    "    return t\n",
    "\n",
    "#The method assigns target for validation and test dataset. \n",
    "# 10% of the actual data (validation data) is used here\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize \n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    return t\n",
    "\n",
    "# This method constructs the design matrix for validation and test data\n",
    "# 10% of the actual data (validation data) is used here\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    return dataMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing Training dataset\n",
    "TrainingTarget_human_concatenation = np.array(GenerateTrainingTarget(RawTarget_human_concatenation))\n",
    "TrainingTarget_human_subtraction = np.array(GenerateTrainingTarget(RawTarget_human_subtraction))\n",
    "TrainingTarget_gradient_concatenation = np.array(GenerateTrainingTarget(RawTarget_gradient_concatenation))\n",
    "TrainingTarget_gradient_subtraction = np.array(GenerateTrainingTarget(RawTarget_gradient_subtraction))\n",
    "\n",
    "\n",
    "#It generates a matrix for training set\n",
    "TrainingData_human_concatenation   = GenerateTrainingDataMatrix(RawData_human_concatenation)\n",
    "TrainingData_human_subtraction   = GenerateTrainingDataMatrix(RawData_human_subtraction)\n",
    "TrainingData_gradient_concatenation   = GenerateTrainingDataMatrix(RawData_gradient_concatenation)\n",
    "TrainingData_gradient_subtraction   = GenerateTrainingDataMatrix(RawData_gradient_subtraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing validating dataset\n",
    "\n",
    "ValDataAct_human_concatenation = np.array(GenerateValTargetVector(RawTarget_human_concatenation,ValidationPercent, (len(TrainingTarget_human_concatenation))))\n",
    "ValDataAct_human_subtraction = np.array(GenerateValTargetVector(RawTarget_human_subtraction,ValidationPercent, (len(TrainingTarget_human_subtraction))))\n",
    "ValDataAct_gradient_concatenation = np.array(GenerateValTargetVector(RawTarget_gradient_concatenation,ValidationPercent, (len(TrainingTarget_gradient_concatenation))))\n",
    "ValDataAct_gradient_subtraction = np.array(GenerateValTargetVector(RawTarget_gradient_subtraction,ValidationPercent, (len(TrainingTarget_gradient_subtraction))))\n",
    "\n",
    "\n",
    "ValData_human_concatenation    = GenerateValData(RawData_human_concatenation,ValidationPercent, (len(TrainingTarget_human_concatenation)))\n",
    "ValData_human_subtraction    = GenerateValData(RawData_human_subtraction,ValidationPercent, (len(TrainingTarget_human_subtraction)))\n",
    "ValData_gradient_concatenation    = GenerateValData(RawData_gradient_concatenation,ValidationPercent, (len(TrainingTarget_gradient_concatenation)))\n",
    "ValData_gradient_subtraction    = GenerateValData(RawData_gradient_subtraction,ValidationPercent, (len(TrainingTarget_gradient_subtraction)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Preparing Testing dataset\n",
    "TestDataAct_human_concatenation = np.array(GenerateValTargetVector(RawTarget_human_concatenation,TestPercent, (len(TrainingTarget_human_concatenation)+len(ValDataAct_human_concatenation))))\n",
    "TestDataAct_human_subtraction = np.array(GenerateValTargetVector(RawTarget_human_subtraction,TestPercent, (len(TrainingTarget_human_subtraction)+len(ValDataAct_human_subtraction))))\n",
    "TestDataAct_gradient_concatenation = np.array(GenerateValTargetVector(RawTarget_gradient_concatenation,TestPercent, (len(TrainingTarget_gradient_concatenation)+len(ValDataAct_gradient_concatenation))))\n",
    "TestDataAct_gradient_subtraction = np.array(GenerateValTargetVector(RawTarget_gradient_subtraction,TestPercent, (len(TrainingTarget_gradient_subtraction)+len(ValDataAct_gradient_subtraction))))\n",
    "\n",
    "\n",
    "\n",
    "#It generates a matrix for validation set\n",
    "TestData_human_concatenation = GenerateValData(RawData_human_concatenation,TestPercent, (len(TrainingTarget_human_concatenation)+len(ValDataAct_human_concatenation)))\n",
    "TestData_human_subtraction = GenerateValData(RawData_human_subtraction,TestPercent, (len(TrainingTarget_human_subtraction)+len(ValDataAct_human_subtraction)))\n",
    "TestData_gradient_concatenation = GenerateValData(RawData_gradient_concatenation,TestPercent, (len(TrainingTarget_gradient_concatenation)+len(ValDataAct_gradient_concatenation)))\n",
    "TestData_gradient_subtraction = GenerateValData(RawData_gradient_subtraction,TestPercent, (len(TrainingTarget_gradient_subtraction)+len(ValDataAct_gradient_subtraction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Kmeans cluster stores M centroids\n",
    "#We have M number of individual clusters, and M centroids, with each  \n",
    "#Mu contains centroids for each basis function, containing detail about all features  \n",
    "#Mu is the cordinate of centroid for each cluster\n",
    "kmeans_human_concatenation = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData_human_concatenation))\n",
    "Mu_human_concatenation = kmeans_human_concatenation.cluster_centers_\n",
    "\n",
    "kmeans_human_subtraction = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData_human_subtraction))\n",
    "Mu_human_subtraction = kmeans_human_subtraction.cluster_centers_\n",
    "\n",
    "kmeans_gradient_concatenation = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData_gradient_concatenation))\n",
    "Mu_gradient_concatenation = kmeans_gradient_concatenation.cluster_centers_\n",
    "\n",
    "kmeans_gradient_subtraction = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData_gradient_subtraction))\n",
    "Mu_gradient_subtraction = kmeans_gradient_subtraction.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This method generates the variance matrix. It contains \n",
    "#So in order to make matrix multiplication feasible, we expand the variance vector (varVect)\n",
    "#Only the diagonal is filled and rest is zero entries. \n",
    "#Thus the bigsigma contains only diagonal entries (variance) and the rest are zero (covariance).\n",
    "\n",
    "def GenerateBigSigma(Data,TrainingPercent,flag):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    #Calculate the transpose of RawData, to get matrix of dimensions entries*features\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))    \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])\n",
    "            #Returns the variance of the array elements\n",
    "        varVect.append(np.var(vct))\n",
    "    #Varvect contains the variance\n",
    "    #Bigsigma is feature*feature matrix, extended from varvect. Bigsigma only has diagonal filled and rest is zero\n",
    "    if(flag == 0):\n",
    "        for j in range(len(Data)):\n",
    "            BigSigma[j][j] = varVect[j]\n",
    "    else:\n",
    "        for j in range(len(Data)):\n",
    "            BigSigma[j][j] = varVect[j] + 0.25\n",
    "    BigSigma = np.dot(100,BigSigma)\n",
    "    #Bigsigma returns the variance matrix\n",
    "    return BigSigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variance matrix is found using Bigsigma method \n",
    "#Variance being a scalar, we generate a Matrix by multiplying variance value for each feature into Identity vector\n",
    "#Bigsigma is feature*feature matrix, extended from varvect. Bigsigma only has diagonal filled and rest is zero\n",
    "BigSigma_human_concatenation     = GenerateBigSigma(RawData_human_concatenation, TrainingPercent,0)\n",
    "BigSigma_human_subtraction     = GenerateBigSigma(RawData_human_subtraction, TrainingPercent,0)\n",
    "BigSigma_gradient_concatenation     = GenerateBigSigma(RawData_gradient_concatenation, TrainingPercent,1)\n",
    "BigSigma_gradient_subtraction     = GenerateBigSigma(RawData_gradient_subtraction, TrainingPercent,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the scalar product of Gaussian radial basis functions\n",
    "# Finds  (((x − µj)^-1) (Σ(^-1)(x − µj))) value\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):\n",
    "    # Find the Gaussian radial basis functions  is φj (x)\n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "\n",
    "\n",
    "# GetPhiMatrix finds Gaussian radial basis functions  is φj (x) = exp (−1/2 (((x − µj)^-1) (Σ(^-1)(x − µj)))) \n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    #Datatranspose is transpose matrix of RawData\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))\n",
    "    # PHI is filled with zeros with dimensions of TrainingLen and MuMatrix \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix)))\n",
    "    # BigSigInv is the inverse of BigSigma of same dimensions feature*feature\n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            # For M values of Mumatrix and all sample values of Training, generate PHI\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    return PHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_PHI_human_concatenation = GetPhiMatrix(RawData_human_concatenation, Mu_human_concatenation, BigSigma_human_concatenation, TrainingPercent)\n",
    "TRAINING_PHI_human_subtraction = GetPhiMatrix(RawData_human_subtraction, Mu_human_subtraction, BigSigma_human_subtraction, TrainingPercent)\n",
    "TRAINING_PHI_gradient_concatenation = GetPhiMatrix(RawData_gradient_concatenation, Mu_gradient_concatenation, BigSigma_gradient_concatenation, TrainingPercent)\n",
    "TRAINING_PHI_gradient_subtraction = GetPhiMatrix(RawData_gradient_subtraction, Mu_gradient_subtraction, BigSigma_gradient_subtraction, TrainingPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pseudo inverse is found, because the matrix is not a square singular matrix. So  Moore-Penrose pseudo-inverse formula is applied \n",
    "# w∗ = (λI + (Φ^T)Φ)^(-1))(Φ^T)t\n",
    "\n",
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    #I is created using the np.identity function as a M*M matrix.\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    #λI is determined for the I matrix of size M*M\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    # Φ^(-1) is found using np functional calls\n",
    "    # Finds the transpose of the Φ and has dimensions (M, sample)\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    # (Φ^(-1)).Φ is determined\n",
    "    # (Φ^(-1)).Φ is reduced to a (M, M) matrix\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    # (λI + (Φ^T)Φ is calculated \n",
    "    # Again PHI_SQR_LI becomes (M, M) matrix\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    # (λI + (Φ^T)Φ ^(-1)\n",
    "    # PHI_SQR_INV is again a matrix of size (M, M)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    # (λI + (Φ^T)Φ)^(-1))(Φ^T)\n",
    "    # INTER becomes (M, sample) because Φ^T is (M, sample)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    # (λI + (Φ^T)Φ)^(-1))(Φ^T)t is determined\n",
    "    # w becomes (M,) because of the dot product being involved\n",
    "    W           = np.dot(INTER, T)\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate weights for the training dataset\n",
    "W_human_concatenation  = GetWeightsClosedForm(TRAINING_PHI_human_concatenation,TrainingTarget_human_concatenation,(C_Lambda))\n",
    "W_human_subtraction  = GetWeightsClosedForm(TRAINING_PHI_human_subtraction,TrainingTarget_human_subtraction,(C_Lambda))\n",
    "\n",
    "#Calculate weights for the training dataset\n",
    "W_gradient_concatenation  = GetWeightsClosedForm(TRAINING_PHI_gradient_concatenation,TrainingTarget_gradient_concatenation,(C_Lambda))\n",
    "W_gradient_subtraction  = GetWeightsClosedForm(TRAINING_PHI_gradient_subtraction,TrainingTarget_gradient_subtraction,(C_Lambda))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the PHI matrix for test and validation set for all 4 cases\n",
    "TEST_PHI_human_concatenation     = GetPhiMatrix(TestData_human_concatenation, Mu_human_concatenation, BigSigma_human_concatenation, 100)\n",
    "VAL_PHI_human_concatenation      = GetPhiMatrix(ValData_human_concatenation, Mu_human_concatenation, BigSigma_human_concatenation, 100)\n",
    "\n",
    "TEST_PHI_human_subtraction     = GetPhiMatrix(TestData_human_subtraction, Mu_human_subtraction, BigSigma_human_subtraction, 100)\n",
    "VAL_PHI_human_subtraction      = GetPhiMatrix(ValData_human_subtraction, Mu_human_subtraction, BigSigma_human_subtraction, 100)\n",
    "\n",
    "\n",
    "TEST_PHI_gradient_concatenation     = GetPhiMatrix(TestData_gradient_concatenation, Mu_gradient_concatenation, BigSigma_gradient_concatenation, 100)\n",
    "VAL_PHI_gradient_concatenation      = GetPhiMatrix(ValData_gradient_concatenation, Mu_gradient_concatenation, BigSigma_gradient_concatenation, 100)\n",
    "\n",
    "TEST_PHI_gradient_subtraction     = GetPhiMatrix(TestData_gradient_subtraction, Mu_gradient_subtraction, BigSigma_gradient_subtraction, 100)\n",
    "VAL_PHI_gradient_subtraction      = GetPhiMatrix(ValData_gradient_subtraction, Mu_gradient_subtraction, BigSigma_gradient_subtraction, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the error mean square value\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (1,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        # Compare the target value and predicted value\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SGC has large datasamples, which would be ignored by the ususal SGD, so we consider batching to use all data samples \n",
    "#The weights are updated after each batch\n",
    "def SGD_batch(filename,W,TrainingTarget,TRAINING_PHI,VAL_PHI,TEST_PHI,ValDataAct,TestDataAct):\n",
    "       \n",
    "    #Use the weights from SGD method\n",
    "    W_Now        = np.dot(220, W)\n",
    "    La           = 2\n",
    "    \n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    W_Mat        = []\n",
    "    epoch = 500\n",
    "    batch_size = 100\n",
    "    \n",
    "    \n",
    "    for j in range(1,epoch):     \n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        learningRate = 1/j\n",
    "        for i in range(int(len(TrainingTarget) / batch_size)):\n",
    "            Delta_E_D = - np.dot((TrainingTarget[start:end] - np.dot(np.transpose(W_Now),np.transpose(TRAINING_PHI[start:end]))),TRAINING_PHI[start:end])\n",
    "            La_Delta_E_W  = np.dot(La,W_Now)\n",
    "            Delta_E       = np.add(Delta_E_D,La_Delta_E_W)\n",
    "            #Error Delta value would increase, so we find mean to minimize this overshoot\n",
    "            mean = len(TrainingTarget)/batch_size\n",
    "            Delta_E = Delta_E / int(mean)            \n",
    "            Delta_W = -np.dot(learningRate,Delta_E)\n",
    "            W_T_Next = W_Now + Delta_W\n",
    "            W_Now    = W_T_Next\n",
    "            start = start + batch_size\n",
    "            end = end + batch_size\n",
    "               \n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "        L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "        L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,TestDataAct)\n",
    "        L_Erms_Test.append(float(Erms_Test.split(',')[1]))    \n",
    "    \n",
    "\n",
    "    print (filename + \" Accuracy Training   = \" + str(float(Erms_TR.split(',')[0])))\n",
    "    print (filename + \" Accuracy Validation = \" + str(float(Erms_Val.split(',')[0])))\n",
    "    print (filename + \" Accuracy Testing    = \" + str(float(Erms_Test.split(',')[0])))\n",
    "    print (\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(filename,W,TrainingTarget,TRAINING_PHI,VAL_PHI,TEST_PHI,ValDataAct,TestDataAct):\n",
    "    #Update the weights SGD method\n",
    "    W_Now        = np.dot(220, W)\n",
    "    La           = 2\n",
    "    \n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    W_Mat        = []\n",
    "\n",
    "    for i in range(1,400):\n",
    "        learningRate = 1/i\n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "        La_Delta_E_W  = np.dot(La,W_Now)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "        W_T_Next      = W_Now + Delta_W\n",
    "        W_Now         = W_T_Next\n",
    "\n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "        L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "        L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,TestDataAct)\n",
    "        L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "\n",
    "    print (filename + \" Accuracy Training   = \" + str(float(Erms_TR.split(',')[0])))\n",
    "    print (filename + \" Accuracy Validation = \" + str(float(Erms_Val.split(',')[0])))\n",
    "    print (filename + \" Accuracy Testing    = \" + str(float(Erms_Test.split(',')[0])))\n",
    "    print (\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Linear Regression Gradient Descent Solution--------------------\n",
      "human_concatenation Accuracy Training   = 50.07898894154818\n",
      "human_concatenation Accuracy Validation = 49.36708860759494\n",
      "human_concatenation Accuracy Testing    = 49.044585987261144\n",
      "-------------------------------------------------------------------------\n",
      "human_subtraction Accuracy Training   = 49.842022116903635\n",
      "human_subtraction Accuracy Validation = 47.46835443037975\n",
      "human_subtraction Accuracy Testing    = 54.140127388535035\n",
      "-------------------------------------------------------------------------\n",
      "gradient_concatenation Accuracy Training   = 54.5\n",
      "gradient_concatenation Accuracy Validation = 56.45645645645646\n",
      "gradient_concatenation Accuracy Testing    = 55.85585585585586\n",
      "-------------------------------------------------------------------------\n",
      "gradient_subtraction Accuracy Training   = 72.6875\n",
      "gradient_subtraction Accuracy Validation = 73.47347347347348\n",
      "gradient_subtraction Accuracy Testing    = 74.57457457457457\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print ('----------Linear Regression Gradient Descent Solution--------------------')\n",
    "SGD('human_concatenation',W_human_concatenation,TrainingTarget_human_concatenation,TRAINING_PHI_human_concatenation,VAL_PHI_human_concatenation,TEST_PHI_human_concatenation,ValDataAct_human_concatenation,TestDataAct_human_concatenation)\n",
    "SGD('human_subtraction',W_human_subtraction,TrainingTarget_human_subtraction,TRAINING_PHI_human_subtraction,VAL_PHI_human_subtraction,TEST_PHI_human_subtraction,ValDataAct_human_subtraction,TestDataAct_human_subtraction)\n",
    "\n",
    "SGD_batch('gradient_concatenation',W_gradient_concatenation,TrainingTarget_gradient_concatenation,TRAINING_PHI_gradient_concatenation,VAL_PHI_gradient_concatenation,TEST_PHI_gradient_concatenation,ValDataAct_gradient_concatenation,TestDataAct_gradient_concatenation)\n",
    "SGD_batch('gradient_subtraction',W_gradient_subtraction,TrainingTarget_gradient_subtraction,TRAINING_PHI_gradient_subtraction,VAL_PHI_gradient_subtraction,TEST_PHI_gradient_subtraction,ValDataAct_gradient_subtraction,TestDataAct_gradient_subtraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sigmoid method is implemented \n",
    "def sigmoid(val):\n",
    "    return 1 / (1 + np.exp(-val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find the accuracy of the model\n",
    "def logistic_accuracy(features,labels,weight,filename):\n",
    "    #The predicted value is the product of feature and weight. The bias is added to the feature set initially\n",
    "    #Bias is a row filled with ones\n",
    "    accuracy_list = np.dot(np.hstack((np.ones((features.shape[0], 1)),features)), weight)\n",
    "    #The predicted unnormalized value is passed to sigmoid method\n",
    "    predicted_list = np.round(sigmoid(accuracy_list))\n",
    "    #If the predicted matches the actual target then their count is counted\n",
    "    accuracy = (predicted_list == labels).sum().astype(float) / len(predicted_list)\n",
    "    print(\" Logistic regression accuracy of \" + str(filename) + ' is ' + str(accuracy))\n",
    "    return accuracy_list, predicted_list, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate target method extracts the features and labels into 2 seperate lists\n",
    "def generate_target(filename,delimiter_value):\n",
    "    file_raw_data = genfromtxt(filename, delimiter=',')\n",
    "    features,emptyarray,labels_temp = np.hsplit(file_raw_data, np.array([int(delimiter_value), int(delimiter_value)]))\n",
    "    labels = np.asarray(np.asarray(labels_temp).ravel().tolist(), dtype=np.float32)\n",
    "    #labels and features are the two required nparray\n",
    "    print(str(filename) + \" is processed for logistic regression\")\n",
    "    return features,labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method constructs the design matrix for training data\n",
    "# 90% of the actual data (test data) is used construct the feature datamatrix\n",
    "def LogisticGenerateTraining(rawTraining):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(0.90)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    return t\n",
    "\n",
    "\n",
    "#The method assigns target and feature for est dataset. \n",
    "# 10% of the actual data is used here\n",
    "def LogisticGenerateTesting(rawData, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*0.10))\n",
    "    V_End = TrainingCount + valSize \n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This method finds the weight gradient and returns it\n",
    "def find_logistic_gradient(features, labels):\n",
    "    bias = np.ones((features.shape[0], 1))\n",
    "    features = np.hstack((bias, features))\n",
    "        \n",
    "    weight = np.zeros(features.shape[1])\n",
    "    for i in range(1,500):\n",
    "        learning_rate = 1 / i\n",
    "        unnormalized_prediction = np.dot(features, weight)\n",
    "        prediction = sigmoid(unnormalized_prediction)\n",
    "\n",
    "        # Update weights with log likelihood gradient\n",
    "        gradient = np.dot(features.T, (labels - prediction))\n",
    "        weight += learning_rate * gradient\n",
    "        # Print log-likelihood every so often\n",
    "    #print(\" The final log likelihood is  \" + str(log_likelihood_function(features, labels, weight)))\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-cleaned/human-feature-concatenation.csv is processed for logistic regression\n",
      "Dataset-cleaned/human-feature-subtraction.csv is processed for logistic regression\n",
      "Dataset-cleaned/gsc-feature-concatenation.csv is processed for logistic regression\n",
      "Dataset-cleaned/gsc-feature-subtraction.csv is processed for logistic regression\n"
     ]
    }
   ],
   "source": [
    "#Generate features and labels\n",
    "human_con_features,human_con_labels = generate_target('Dataset-cleaned/human-feature-concatenation.csv',18)\n",
    "human_sub_features,human_sub_labels = generate_target('Dataset-cleaned/human-feature-subtraction.csv',9)\n",
    "\n",
    "gsc_con_features,gsc_con_labels = generate_target('Dataset-cleaned/gsc-feature-concatenation.csv',1024)\n",
    "gsc_sub_features,gsc_sub_labels = generate_target('Dataset-cleaned/gsc-feature-subtraction.csv',512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Preparing Training dataset\n",
    "human_con_labels_train = np.array(LogisticGenerateTraining(human_con_labels))\n",
    "human_sub_labels_train = np.array(LogisticGenerateTraining(human_sub_labels))\n",
    "gsc_con_labels_train = np.array(LogisticGenerateTraining(gsc_con_labels))\n",
    "gsc_sub_labels_train = np.array(LogisticGenerateTraining(gsc_sub_labels))\n",
    "\n",
    "\n",
    "#It generates a matrix for training set\n",
    "human_con_features_train   = np.array(LogisticGenerateTraining(human_con_features))\n",
    "human_sub_features_train   = np.array(LogisticGenerateTraining(human_sub_features))\n",
    "gsc_con_features_train   = np.array(LogisticGenerateTraining(gsc_con_features))\n",
    "gsc_sub_features_train   = np.array(LogisticGenerateTraining(gsc_sub_features))\n",
    "\n",
    "\n",
    "#Preparing Testing dataset\n",
    "human_con_labels_test = np.array(LogisticGenerateTesting(human_con_labels,len(human_con_labels_train)))\n",
    "human_sub_labels_test = np.array(LogisticGenerateTesting(human_sub_labels,len(human_sub_labels_train)))\n",
    "gsc_con_labels_test = np.array(LogisticGenerateTesting(gsc_con_labels,len(gsc_con_labels_train)))\n",
    "gsc_sub_labels_test = np.array(LogisticGenerateTesting(gsc_sub_labels,len(gsc_sub_labels_train)))\n",
    "\n",
    "\n",
    "#It generates a matrix for Testing set\n",
    "human_con_features_test   = np.array(LogisticGenerateTesting(human_con_features,len(human_con_features_train)))\n",
    "human_sub_features_test   = np.array(LogisticGenerateTesting(human_sub_features,len(human_sub_features_train)))\n",
    "gsc_con_features_test   = np.array(LogisticGenerateTesting(gsc_con_features,len(gsc_con_features_train)))\n",
    "gsc_sub_features_test   = np.array(LogisticGenerateTesting(gsc_sub_features,len(gsc_sub_features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate weights \n",
    "w_human_con = find_logistic_gradient(human_con_features_train,human_con_labels_train)\n",
    "w_human_sub = find_logistic_gradient(human_sub_features_train,human_sub_labels_train)\n",
    "w_gsc_con = find_logistic_gradient(gsc_con_features_train,gsc_con_labels_train)\n",
    "w_gsc_sub = find_logistic_gradient(gsc_sub_features_train,gsc_sub_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic regression accuracy of Human concatenation is 0.5031847133757962\n",
      " Logistic regression accuracy of Human subtraction is 0.47770700636942676\n",
      " Logistic regression accuracy of GSC concatenation is 0.7947947947947948\n",
      " Logistic regression accuracy of GSC subtraction is 0.5865865865865866\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy\n",
    "accuracy_human_con = logistic_accuracy(human_con_features_test,human_con_labels_test,w_human_con,'Human concatenation')\n",
    "accuracy_human_sub = logistic_accuracy(human_sub_features_test,human_sub_labels_test,w_human_sub,'Human subtraction')\n",
    "accuracy_gsc_con = logistic_accuracy(gsc_con_features_test,gsc_con_labels_test,w_gsc_con,'GSC concatenation')\n",
    "accuracy_gsc_sub = logistic_accuracy(gsc_sub_features_test,gsc_sub_labels_test,w_gsc_sub,'GSC subtraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create column header for the input dataset\n",
    "def create_array_header(chars,position):\n",
    "    em = []\n",
    "    for j in chars:\n",
    "        for i in range(position):\n",
    "            em.append(j+str(i))\n",
    "    em.append('target')\n",
    "    return em        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate result for vectors\n",
    "#Results are made into a list from a int, because of hot shot encoding\n",
    "def create_target_array(original_array):\n",
    "    iterable = []\n",
    "    for iter_item in original_array:\n",
    "        iterator = []\n",
    "        if(iter_item == 1):\n",
    "            iterator = iterator + [0,1] \n",
    "        else:\n",
    "            iterator = iterator + [1,0]\n",
    "        iterable.append(iterator)\n",
    "    return iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract the features from the files and shuffle/split the file into training and test data sets \n",
    "def tf_feature(filename,columnnames):\n",
    "    filecontent = pd.read_csv(filename)\n",
    "    filecontent.columns = columnnames\n",
    "    feature_id = filecontent.drop(labels=['target'], axis=1).values\n",
    "    feature_target = filecontent.target.values\n",
    "    \n",
    "    np.random.seed(5)\n",
    "    tf.set_random_seed(5)\n",
    "    #replace is set as False to avoid sampling twice\n",
    "    train_index = np.random.choice(len(feature_id), round(len(feature_id) * 0.75), replace=False)\n",
    "    test_index = np.array(list(set(range(len(feature_id))) - set(train_index)))\n",
    "    train_feature = (feature_id[train_index])\n",
    "    train_target = (feature_target[train_index])\n",
    "    test_feature = (feature_id[test_index])\n",
    "    test_target = (feature_target[test_index])    \n",
    "    return train_feature,train_target,test_feature,test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensorflow_result(array_sequence,filename, position,final_append):\n",
    "    train_feature,train_target_temp,test_feature,test_target_temp = tf_feature(filename, array_sequence)\n",
    "    train_target = create_target_array(train_target_temp)\n",
    "    test_target = create_target_array(test_target_temp)\n",
    "    training_epochs = 200\n",
    "    batch_size = 100\n",
    "    #the train and test sets are obtained\n",
    "    #batch and epochs are initializd\n",
    "    #hidden layer is initialized with no of features\n",
    "    hidden_layer_1 = int(position) \n",
    "    #hidden layer is initialized with no of features\n",
    "    input_layer = int(position)\n",
    "    #There are 2 prediction classes, so it is 2\n",
    "    classes = 2\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    #initialized the  input of tfgraph\n",
    "    x = tf.placeholder(\"float\", [None, input_layer])\n",
    "    y = tf.placeholder(\"float\", [None, classes])\n",
    "\n",
    "    #set values for weight and bias (input and output later)\n",
    "    weights = {'h1': tf.Variable(tf.random_normal([input_layer, hidden_layer_1])),'out': tf.Variable(tf.random_normal([hidden_layer_1, classes]))}\n",
    "    biases = {'b1': tf.Variable(tf.random_normal([hidden_layer_1])),'out': tf.Variable(tf.random_normal([classes]))}\n",
    "\n",
    "    #find the hidden layer. the product of input, weight and bias\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    #this is passed to sigmoid method\n",
    "    \n",
    "    #This is the output later, prod of weight, bias abd hiddenlayer\n",
    "    pred = tf.matmul(layer_1, weights['out']) + biases['out'] \n",
    "    \n",
    "    #choose an appropriate cost and optimizer method\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    #initializing the session\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        #training model\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0\n",
    "            #extract the feature and target of the batch and find the cost and optimization\n",
    "            total_batch = int(len(train_feature)/batch_size)\n",
    "            train_feature_batch = np.array_split(train_feature, total_batch)\n",
    "            train_target_batch = np.array_split(train_target, total_batch)\n",
    "            for i in range(total_batch):\n",
    "                batch_feature, batch_target = train_feature_batch[i], train_target_batch[i]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_feature,y: batch_target})\n",
    "            avg_cost += c / total_batch\n",
    "            #this is the average loss of the iteration\n",
    "\n",
    "        #test the model built and find its accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(\"Tensorflow Accuracy for \" + final_append + \" \", accuracy.eval({x: test_feature, y: test_target}))\n",
    "        global result \n",
    "        result = tf.argmax(pred, 1).eval({x: test_feature, y: test_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-46-1b7d6f066694>:34: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Tensorflow Accuracy for Human Concatination  0.5164557\n",
      "Tensorflow Accuracy for Human Subtraction  0.5493671\n",
      "Tensorflow Accuracy for GSC Concatenation  0.7616\n",
      "Tensorflow Accuracy for GSC Subtraction  0.5964\n"
     ]
    }
   ],
   "source": [
    "array_sequence = create_array_header(['a','b'],9)\n",
    "tensorflow_result(array_sequence,\"Dataset-cleaned/human-feature-concatenation.csv\",18,'Human Concatination')\n",
    "array_sequence = create_array_header(['a'],9)\n",
    "tensorflow_result(array_sequence,\"Dataset-cleaned/human-feature-subtraction.csv\",9,'Human Subtraction')\n",
    "array_sequence = create_array_header(['a','b'],512)\n",
    "tensorflow_result(array_sequence,\"Dataset-cleaned/gsc-feature-concatenation.csv\",1024,'GSC Concatenation')\n",
    "array_sequence = create_array_header(['a'],512)\n",
    "tensorflow_result(array_sequence,\"Dataset-cleaned/gsc-feature-subtraction.csv\",512,'GSC Subtraction')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
